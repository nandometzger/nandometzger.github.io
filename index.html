<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0"> -->

    <title>Nando Metzger</title>

    <meta name="author" content="Nando Metzger">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="shortcut icon" href="assets/images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5FBRP8LCRB"></script>
  <script>
    window.dataLayer = window.dataLayer || []; 
    function gtag() {
      dataLayer.push(arguments);
    } 
    gtag('js', new Date()); 
    gtag('config', 'G-5FBRP8LCRB');
  </script>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Nando Metzger
                </p>
                <p>
                  I am a PhD student in the
                  <a style="display:inline" href="https://prs.igp.ethz.ch/">Photogrammetry and Remote Sensing Lab at ETH Z√ºrich</a>.
                  My supervisors are 
                  <a style="display:inline" href="https://scholar.google.ch/citations?user=FZuNgqIAAAAJ&hl=en">Prof. Konrad Schindler</a>
                  and 
                  <a style="display:inline" href="https://scholar.google.com/citations?user=p3iJiLIAAAAJ&hl=en">Prof. Devis Tuia</a>.
		  Currently, I am a Student Researcher at Google working on 3D computer vision.
		</p>
                <p> 
		  I like to work on computer vision problems with various applications such as monocular depth estimation, super-resolution, and remote sensing.
		  I collaborated with the <a style="display:inline" href="https://www.icrc.org/en">ICRC</a> to map vulnerable populations in developing countries.
                  In the past, I interned and collaborated with <a style="display:inline" href="https://about.meta.com/realitylabs/">Meta's Reality Labs Research</a>.
                </p>
                <p> 
                  I obtained my bachelor's and master's degree in Geomatics Engineering from ETH Z√ºrich. During my master's I specialized in deep learning, computer vision and remote sensing.
                </p>
                <p style="text-align:center">
                  <a href="mailto:nando.metzger@geod.baug.ethz.ch">Email</a> &nbsp;/&nbsp; 
                  <a href="https://scholar.google.ch/citations?hl=en&user=pGqbcuIAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/nando-metzger-68aa0518b/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/nandometzger">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/nandometzger/">Github</a> &nbsp;/&nbsp;
                  <a href="https://medium.com/@nandometzger">Medium</a>
                </p>
                  <p style="text-align:center; color: red;">
                    I will graduate in January 2026 and am actively seeking full-time opportunities in industry. If you‚Äôre hiring or know of relevant opportunities, I‚Äôd love to connect.
                  </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="./assets/images/Nando_Portrait.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="./assets/images/Nando_Portrait.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="17">
            <tr> 
              <td width="10%" valign="middle">
                <a href="https://ethz.ch/en.html"><img style="width:100%;max-width:100%;object-fit: cover;" src="assets/images/eth_logo.png"></a>
              </td>
              <td width="5%" valign="middle">
                <a href="https://prs.igp.ethz.ch/"><img style="width:100%;max-width:100%;object-fit: cover;" src="assets/images/ICRC_logo.png"></a>
<!--                 <a href="https://prs.igp.ethz.ch/"><img style="width:100%;max-width:100%;object-fit: cover;" src="assets/images/prslogo_only.jpg""></a> -->
              </td>
              <td width="10%" valign="middle">
                <a href="https://about.meta.com/"><img style="width:100%;max-width:100%;object-fit: cover;" src="assets/images/meta_logo.png"></a>
              </td>
<!--               <td width="5%" valign="middle">
                <a href="https://about.meta.com/"><img style="width:100%;max-width:100%;object-fit: cover;" src="assets/images/Reality_Labs_logo.svg.png"></a>
              </td>  -->
              <td width="10%" valign="middle">
                <a href="https://research.google//"><img style="width:100%;max-width:100%;object-fit: cover;" src="assets/images/GoogleLogo.png"></a>
              </td> 
            </tr>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision, deep learning, and their applications to remote sensing.
                  Most of my work is related to super-resolution, depth estimation, or both simultaneously.
                  Some papers are <span class="highlight">highlighted</span>. * indicates equal contribution.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>




		  
            
            <tr onmouseout="marigoldDC_stop()" onmouseover="marigoldDC_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
          <div class="two" id='marigoldDC_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/images/marigold_dc_video.webm" type="video/mp4">
          Your browser does not support the video tag.
        </video></div>
          <img src='assets/images/marigolddc_logo_square.jpg' width="160">
        </div>
        <script type="text/javascript">
          function marigoldDC_start() {
            document.getElementById('marigoldDC_image').style.opacity = "1";
          }

          function marigoldDC_stop() {
            document.getElementById('marigoldDC_image').style.opacity = "0";
          }
          marigoldDC_stop()
          </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://marigolddepthcompletion.github.io/">
			<span class="papertitle">‚áÜ Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion
</span>
</a>
        <br>
	<a href="https://www.linkedin.com/in/massimiliano-viola/">Massimiliano Viola</a>,
	<a href="https://www.linkedin.com/in/kevin-qu-b3417621b/">Kevin Qu</a>,
        <strong>Nando Metzger</strong>, 
	<a href="http://www.kebingxin.com/">Bingxin Ke</a>,
	<a href="https://scholar.google.ch/citations?user=Wle2GmkAAAAJ&hl=en">Alexander Becker</a>,
        <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>
  	<a href="https://www.obukhov.ai/">Anton Obukhov</a>, 

        <br>
        <em>ICCV</em>, 2025 &nbsp <font color="red"><strong></strong></font>
        <br>
        <a href="https://marigolddepthcompletion.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2412.13389">arXiv</a>
        /
        <a href="https://github.com/prs-eth/Marigold-DC">code</a>
        /
        <a href="https://huggingface.co/spaces/prs-eth/marigold-dc">demo</a>
        <p></p>
        <p>
          Marigold-DC is a zero-shot depth completion frame work. We repurpose Marigold as an off-the-shelf monocular depth estimator and guide it with sparse depth observations.
        </p>
      </td>
    </tr>






    <tr onmouseout="damage_stop()" onmouseover="damage_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='damage_image'>
            <img src='assets/damage2.png' width=100%>
          </div>
          <img src='assets/damage.png' width="160">
        </div>
        <script type="text/javascript">
          function damage_start() {
            document.getElementById('damage_image').style.opacity = "1";
          }

          function damage_stop() {
            document.getElementById('damage_image').style.opacity = "0";
          }
          damage_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2511.05461">
          <span class="papertitle">The Potential of Copernicus Satellites for Disaster Response: Retrieving Building Damage from Sentinel-1 and Sentinel-2</span>
        </a>
        <br>
        Olivier Dietrich,
        Merlin Alfredsson,
        Emilia Arens,
        <strong>Nando Metzger</strong>,
        <a href="https://petertor.github.io/">Torben Peters</a>,
        <a href="https://scheibenreif.github.io/">Linus Scheibenreif</a>,
        <a href="https://scholar.google.ch/citations?user=sxLG1rgAAAAJ&hl=en">Jan Dirk Wegner</a>,
        <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>
        <br>
        <em>arXiv</em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2511.05461">arXiv</a>
        <!-- /
        <a href="https://doi.org/10.48550/arXiv.2511.05461">DOI</a> -->
        <!-- /
        <a href="https://huggingface.co/papers/2511.05461">HF paper page</a> -->
        <p></p>
        <p>
          We investigate whether medium-resolution Copernicus Sentinel-1 and Sentinel-2 imagery can support rapid building damage assessment after disasters.
          We introduce the xBD-S12 dataset and show that, despite 10&nbsp;m resolution, building damage can be mapped reliably across many events, making Copernicus data a practical complement to limited very-high resolution imagery.
        </p>
      </td>
    </tr>

		  

    
    <tr onmouseout="circumpolarLST_stop()" onmouseover="circumpolarLST_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='circumpolarLST_image'>
            <img src='assets/temp2.png' width=100%>
          </div>
          <img src='assets/temp1.png' width="160">
        </div>
        <script type="text/javascript">
          function circumpolarLST_start() {
            document.getElementById('circumpolarLST_image').style.opacity = "1";
          }

          function circumpolarLST_stop() {
            document.getElementById('circumpolarLST_image').style.opacity = "0";
          }
          circumpolarLST_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">Four decades of circumpolar super-resolved satellite land surface temperature data</span>
        <br>
        <a href="https://www.geography.unibe.ch/about_us/staff/dupuis_sonia/index_eng.html">Sonia Dupuis</a>,
        <strong>Nando Metzger</strong>,
        <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>,
        Frank G√∂ttsche,
        Stefan Wunderle
        <br>
        <em>arXiv submission</em>, 2025
        <p></p>
        <p>
          We present a 42-year pan-Arctic land surface temperature dataset, downscaled from AVHRR GAC to 1&nbsp;km resolution with a deep anisotropic diffusion super-resolution model trained on MODIS LST and guided by high-resolution land cover, elevation, and vegetation height.
          The resulting twice-daily, 1&nbsp;km LST record enables improved permafrost and near-surface air temperature modelling, Greenland Ice Sheet surface mass balance assessment, and climate monitoring in the pre-MODIS era.
        </p>
      </td>
    </tr>
    
           
            <tr onmouseout="marigoldTPAMI_stop()" onmouseover="marigoldTPAMI_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
          <div class="two" id='marigoldTPAMI_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/marigold_pami_512.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video></div>
          <img src='assets/images/marigold_pami_512_frame0.jpg' width="160">
        </div>
        <script type="text/javascript">
          function marigoldTPAMI_start() {
            document.getElementById('marigoldTPAMI_image').style.opacity = "1";
          }

          function marigoldTPAMI_stop() {
            document.getElementById('marigoldTPAMI_image').style.opacity = "0";
          }
          marigoldTPAMI_stop()
          </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2505.09358">
			<span class="papertitle">üåºMarigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis
</span>
</a>
        <br>
	<a href="http://www.kebingxin.com/">Bingxin Ke*</a>,
	<a href="https://www.linkedin.com/in/kevin-qu-b3417621b/">Kevin Qu*</a>,
  <a href="https://scholar.google.ch/citations?user=uAAMOnQAAAAJ&hl=en">Tianfu Wang*</a>, 
        <strong>Nando Metzger*</strong>, 
        <a href="https://shengyuh.github.io/">Shengyu Huang</a>,
  <a href="https://scholar.google.com/citations?user=B_jbDpEAAAAJ&hl=en">Bo Li</a>, 
	<a href="https://www.obukhov.ai/">Anton Obukhov</a>, 
        <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>

        <br>
        <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2025 &nbsp <font color="red"><strong></strong></font>
        <br>
        <a href="https://arxiv.org/abs/2505.09358">arXiv</a>
        /
        <a href="https://github.com/prs-eth/marigold">code</a>
        /
        <a href="https://huggingface.co/spaces/prs-eth/marigold">demo</a>
        <p></p>
        <p>
          Marigold (TPAMI) generalizes the original CVPR'24 monocular depth estimator into a diffusion-based foundation model for dense prediction,
          supporting tasks such as depth, surface normals, and intrinsic image decomposition with only a few diffusion steps and efficient fine-tuning.
        </p>
      </td>
    </tr>

            
            <tr onmouseout="marigold_stop()" onmouseover="marigold_start()" bgcolor="#c7eaf2">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
          <div class="two" id='marigold_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/images/marigold_rec.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video></div>
          <img src='assets/images/splashdog_sq.png' width="160">
        </div>
        <script type="text/javascript">
          function marigold_start() {
            document.getElementById('marigold_image').style.opacity = "1";
          }

          function marigold_stop() {
            document.getElementById('marigold_image').style.opacity = "0";
          }
          marigold_stop()
          </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://marigoldmonodepth.github.io/">
			<span class="papertitle">üåºRepurposing Diffusion-Based Image Generators for Monocular Depth Estimation
</span>
</a>
        <br>
	<a href="http://www.kebingxin.com/">Bingxin Ke</a>,
  <a href="https://www.obukhov.ai/">Anton Obukhov</a>, 
        <a href="https://shengyuh.github.io/">Shengyu Huang</a>,
        <strong>Nando Metzger</strong>, 
	<a href="https://rcdaudt.github.io/">Rodrigo Caye Daudt</a>, 
        <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>

        <br>
        <em>CVPR</em>, 2024 &nbsp <font color="red"><strong>(Oral, Best Paper Candidate)</strong></font>
        <br>
        <a href="https://marigoldmonodepth.github.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2312.02145">arXiv</a>
        /
        <a href="https://github.com/prs-eth/marigold">code</a>
        /
        <a href="https://colab.research.google.com/drive/12G8reD13DdpMie5ZQlaFNo2WCGeNUH-u?usp=sharing">colab</a>
        /
        <a href="https://huggingface.co/spaces/prs-eth/marigold-lcm">demo</a>
        <p></p>
        <p>
          Marigold is an affine-invariant monocular depth estimation method based on Stable Diffusion,
          leveraging its rich prior knowledge for better generalization and achieving state-of-the-art performance with significant improvements,
          even with synthetic training data.
        </p>
      </td>
    </tr>
    
    
    
        <tr onmouseout="betterdepth_stop()" onmouseover="betterdepth_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one"> 
         <div class="two" id='betterdepth_image'>
                <img src='assets/BetterDepth_depth.png' width=100%>
              </div>  
              <img src='assets/BetterDepth_cat.png' width="160">
            </div>
            <script type="text/javascript">
              function betterdepth_start() {
                document.getElementById('betterdepth_image').style.opacity = "1";
              }
    
              function betterdepth_stop() {
                document.getElementById('betterdepth_image').style.opacity = "0";
              }
              betterdepth_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://huggingface.co/papers/2407.17952">
          <span class="papertitle">BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular Depth Estimation
    </span>
            </a>
            <br>
      <a href="https://xiangz-0.github.io/">Xiang Zhang</a>, 
      <a href="http://www.kebingxin.com/">Bingxin Ke</a>,
      <a href="https://riemenschneider.hayko.at/">Hayko Riemenschneider</a>, 
            <strong>Nando Metzger</strong>, 
            <a href="https://www.obukhov.ai/">Anton Obukhov</a>, 
            <a href="https://scholar.google.ch/citations?user=uxk0GmUAAAAJ&hl=en">Markus Gross</a>, 
            <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>, 
            <a href="https://scholar.google.ch/citations?user=j0nCulgAAAAJ&hl=en">Christopher Schroers</a>
            <br>
            <em>NeurIPS</em>, 2024
            <br> 
            <a href="https://neurips.cc/virtual/2024/poster/96772">Paper</a>
            /
            <a href="https://arxiv.org/abs/2407.17952">arXiv</a>
            /
            <a href="https://huggingface.co/papers/2407.17952">project</a>
            <p></p>
            <p>
              BetterDepth is a plug-and-play diffusion-based refiner that boosts the performance of any SOTA zero-shot monocular depth estimation method.
            </p>
          </td>
        </tr>
    
        <tr onmouseout="popcorn_stop()" onmouseover="popcorn_start()" bgcolor="#c7eaf2">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='popcorn_image'><video  width=100% muted autoplay loop>
              <source src="assets/images/popcorn_rec.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='assets/images/popcorn_preview.png' width=100%>
            </div>
            <script type="text/javascript">
              function popcorn_start() {
                document.getElementById('popcorn_image').style.opacity = "1";
              }
    
              function popcorn_stop() {
                document.getElementById('popcorn_image').style.opacity = "0";
              }
              popcorn_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://popcorn-population.github.io/">
              <span class="papertitle">üçøPOPCORN: High-resolution Population Maps Derived from Sentinel-1 and Sentinel-2üõ∞Ô∏è
    </span>
            </a>
            <br>
            <strong>Nando Metzger</strong>, 
            <a href="https://rcdaudt.github.io/">Rodrigo Caye Daudt</a>, 
            <a href="https://scholar.google.com/citations?user=p3iJiLIAAAAJ&hl=en">Devis Tuia</a> 
            <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>
            <br>
            <em>Remote Sensing of Environment</em>, 2024
            <br>
            <a href="https://popcorn-population.github.io/">project page</a>
            /
            <a href="https://github.com/prs-eth/popcorn">code</a>
            /
            <a href="https://arxiv.org/abs/2311.14006">arXiv</a>
            /
            <a href="https://ee-nandometzger.projects.earthengine.app/view/popcornv1-rwa">demo</a>
            /
            <a href="https://code.earthengine.google.com/f90c3d3a77ec4dcfeb645457a87ddf48">data</a>
            <p></p>
            <p>
              POPCORN is a lightweight population mapping method using free satellite images and minimal data,
              surpassing existing accuracy and providing interpretable maps for mapping populations in data-scarce regions.
            </p>
          </td>
        </tr>

        <tr onmouseout="snow_stop()" onmouseover="snow_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='snow_image'>
                <img src='assets/fog1.png' width=100%>
              </div>
              <img src='assets/fog2.png' width="160">
            </div>
            <script type="text/javascript">
              function snow_start() {
                document.getElementById('snow_image').style.opacity = "1";
              }

              function snow_stop() {
                document.getElementById('snow_image').style.opacity = "0";
              }
              snow_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://ieeexplore.ieee.org/document/10196748">
              <span class="papertitle">Automatic Image Compositing and Snow Segmentation for Alpine Snow Cover Monitoring</span>
            </a>
            <br>
            Janik Baumer,
            <strong>Nando Metzger</strong>,
            Elisabeth D Hafner,
            <a href="https://rcdaudt.github.io/">Rodrigo Caye Daudt</a>,
            <a href="https://scholar.google.ch/citations?user=sxLG1rgAAAAJ&hl=en">Jan Dirk Wegner</a>,
            <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>
            <br>
            <em>IEEE Swiss Conference on Data Science (SDS)</em>, 2023,
            <br>
            <a href="https://ieeexplore.ieee.org/document/10196748">paper</a>
            <p></p>
            <p>
              We automate SLF's ground-based snow cover monitoring pipeline in the Dischma valley by combining deep learning-based fog classification with pixel-wise snow segmentation for ground camera imagery.
              Our approach removes manual thresholds, generalizes across multiple cameras, and enables more scalable, reliable alpine snow cover mapping to support avalanche research and satellite product validation.
            </p>
          </td>
        </tr>




    <tr onmouseout="thera_stop()" onmouseover="thera_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='thera_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/images/thera_zebra.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='assets/images/thera_zebra_cover.png' width="160">
        </div>
        <script type="text/javascript">
          function thera_start() {
            document.getElementById('thera_image').style.opacity = "1";
          }

          function thera_stop() {
            document.getElementById('thera_image').style.opacity = "0";
          }
          thera_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/prs-eth/thera">
			<span class="papertitle">üî•Thera: Aliasing-Free Arbitrary-Scale Super-Resolution with Neural Heat Fields
</span>
        </a>
        <br>
	<a href="https://github.com/alebeck">Alexander Becker</a>, 
	<a href="https://rcdaudt.github.io/">Rodrigo Caye Daudt</a>, 
        Dominik Narnhofer,
        <a href="https://petertor.github.io/">Torben Peters</a>,
        <strong>Nando Metzger</strong>, 
        <a href="https://scholar.google.ch/citations?user=sxLG1rgAAAAJ&hl=en">Jan Dirk Wegner</a>, 
        <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>
        <br>
        <em>Transactions on Machine Learning Research (TMLR)</em>, 2025
        <br> 
        <a href="https://therasr.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2311.17643">arXiv</a>
        /
        <a href="https://github.com/prs-eth/thera">code</a>
        <p></p>
        <p>
          We introduce neural heat fields, a neural field formulation that inherently models a physically exact point spread function, enabling analytically correct anti-aliasing at any super-resolution scale without extra computation.
          Building on this, Thera achieves aliasing-free arbitrary-scale single image super-resolution, substantially outperforming previous methods while remaining parameter-efficient and supported by strong theoretical guarantees.
        </p>
      </td>
    </tr>



    <tr onmouseout="dada_stop()" onmouseover="dada_start()" bgcolor="#c7eaf2">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='dada_image'><video  width=100% muted autoplay loop>
          <source src="assets/images/diffusion_vid.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='assets/images/dada_sq.png' width=100%>
        </div>
        <script type="text/javascript">
          function dada_start() {
            document.getElementById('dada_image').style.opacity = "1";
          }

          function dada_stop() {
            document.getElementById('dada_image').style.opacity = "0";
          }
          dada_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Metzger_Guided_Depth_Super-Resolution_by_Deep_Anisotropic_Diffusion_CVPR_2023_paper.html">
          <span class="papertitle">ü¶åDADA: Guided Depth Super-Resolution by Deep Anisotropic Diffusion</span>
        </a>
        <br>
	<strong>Nando Metzger</strong>*, 
	<a href="https://rcdaudt.github.io/">Rodrigo Caye Daudt</a>*, 
        <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>
        <br>
        <em>CVPR</em>, 2023
        <br>
        <a href="https://arxiv.org/abs/2211.11592">arXiv</a>
        /
        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Metzger_Guided_Depth_Super-Resolution_by_Deep_Anisotropic_Diffusion_CVPR_2023_paper.html">paper</a>
        /
        <a href="https://rcdaudt.github.io/dada/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=7RgXJz_3kcg">video</a>
        /
        <a href="assets/images/DADA_poster.pdf">poster</a>
        <p></p>
        <p>
          We propose DADA, a novel approach to depth image super-resolution by combining guided anisotropic diffusion with a deep convolutional network, enhancing both edge detail and contextual reasoning.
          This method achieves unprecedented results in three benchmarks, especially at larger scales like x32
        </p>
      </td>
    </tr>


		  
    <tr onmouseout="MLfocal_stop()" onmouseover="MLfocal_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='MLfocal_image'><video  width=100% muted autoplay loop>
          <source src="assets/images/stone_tagged2.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='assets/images/stone_tagged2.png' width=100%>
        </div>
        <script type="text/javascript">
          function MLfocal_start() {
            document.getElementById('MLfocal_image').style.opacity = "1";
          }

          function MLfocal_stop() {
            document.getElementById('MLfocal_image').style.opacity = "0";
          }
          MLfocal_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/nandometzger/MLFocalLengths">
          <span class="papertitle">MLFocalLengths: Estimating the Focal Length of a Single Image</span>
        </a>
        <br>
	<strong>Nando Metzger</strong>  
        <br>
        <em>GitHub Project</em>, 2023
        <br>
        <a href="https://github.com/nandometzger/MLFocalLengths/tree/main">code</a>
        <p></p>
        <p>
          Information about the focal length with which a photo is taken might be obstructed (internet photos) or not available (vintage photos).
	  Inferring the focal length of a photo solely from a monocular view is an ill-posed task that requires knowledge about the scale of objects and their distance to the camera - e.g. scene understanding.
	  I trained a deep learning model to acquire such scene understanding to predict the focal length and open-source the model with this repository.
        </p>
      </td>
    </tr>

    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nuvo_image'><video  width=100% muted autoplay loop>
          <source src="assets/images/marigold_reXc.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='assets/images/pomelo_map2.png' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://pratulsrinivasan.github.io/nuvo/">
          <span class="papertitle">üü°POMELO: Fine-grained Population Mapping from Coarse Census Counts and Open Geodata</span>
        </a>
        <br>
	<strong>Nando Metzger</strong>,
	<a href="https://scholar.google.com.br/citations?user=xbCWyaoAAAAJ&hl=en">John E Vargas-Mu√±oz</a>, 
	<a href="https://rcdaudt.github.io/">Rodrigo Caye Daudt</a>, 
	<a href="https://bkellenb.github.io/">Benjamin Kellenberger</a>,  
          Thao Ton-That Whelan,
          <a href="https://mimran.me/">Muhammad Imran</a>, 
          <a href="https://www.ferdaofli.com/">Ferda Ofli</a>, 
          <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>, 
          <a href="https://scholar.google.com/citations?user=p3iJiLIAAAAJ&hl=en">Devis Tuia</a>
        <br>
        <em>Nature - Scientific Reports</em>, 2022
        <br>
        <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
        /
        <a href="http://arxiv.org/abs/2312.05283">arXiv</a>
        /
        <a href="https://gee-community-catalog.org/projects/pomelo/">community dataset</a>
        <p></p>
        <p>
          POMELO is a deep learning model that creates fine-grained population maps using coarse census counts and open geodata,
          achieving high accuracy in sub-Saharan Africa and effectively estimating population numbers even without any census data.
        </p>
      </td>
    </tr>


    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nuvo_image'><video  width=100% muted autoplay loop>
          <source src="assets/images/marigold_reXc.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='assets/images/forecasting.png' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://link.springer.com/article/10.1007/s41064-023-00258-8">
          <span class="papertitle">Urban Change Forecasting from Satellite Images</span>
        </a>
        <br>
        	<strong>Nando Metzger</strong>,
          <a href="https://scholar.google.com/citations?user=rJpIyQUAAAAJ&hl=en">Mehmet Ozgur Turkoglu</a>
				  <a href="https://rcdaudt.github.io/">Rodrigo Caye Daudt</a>, 
          <a href="https://scholar.google.ch/citations?user=sxLG1rgAAAAJ&hl=en">Jan Dirk Wegner</a>, 
          <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>,
        <br>
        <em>PFG</em>, 2022, &nbsp <font color="red"><strong>(Karl-Kraus Best Paper Award)</strong></font>
        <br>
        <a href="https://link.springer.com/article/10.1007/s41064-023-00258-8">paper</a> 
        <p></p>
        <p>
          We propose a method for forecasting the emergence and timing of new buildings using a deep neural network with a custom pretraining procedure, validated on the SpaceNet7 dataset.
        </p>
      </td>
    </tr>

    
    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nuvo_image'><video  width=100% muted autoplay loop>
          <source src="assets/images/marigold_reXc.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='assets/images/ODERNN.png' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://link.springer.com/article/10.1007/s41064-023-00258-8">
          <span class="papertitle">Crop Classification under Varying Cloud Cover with Neural Ordinary Differential Equations</span>
        </a>
        <br>
        	<strong>Nando Metzger</strong>*,
				  <a href="https://scholar.google.com/citations?user=rJpIyQUAAAAJ&hl=en">Mehmet Ozgur Turkoglu</a>*,
          <a href="https://scholar.google.it/citations?user=vLYzYl4AAAAJ&hl=it">Stefano D'Aronco</a>,
          <a href="https://scholar.google.ch/citations?user=sxLG1rgAAAAJ&hl=en">Jan Dirk Wegner</a>,
          <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>,
        <br>
        <em>IEEE, TGRS</em>, 2021
        <br>
        <a href="https://link.springer.com/article/10.1007/s41064-023-00258-8">paper</a> 
        <p></p>
        <p>
          We propose using neural ordinary differential equations (NODEs) combined with RNNs to improve crop classification from irregularly spaced satellite images,
          showing enhanced accuracy over common methods, especially with few observations,
          and better early-season forecasting due to the continuous representation of latent dynamics.
        </p>
      </td>
    </tr>

    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nuvo_image'><video  width=100% muted autoplay loop>
          <source src="assets/images/marigold_reXc.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='assets/images/DSMref.png' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2012.07427">
          <span class="papertitle">DSM Refinement with Deep Encoder-Decoder Networks</span>
        </a>
        <br>
        	<strong>Nando Metzger</strong>,
          <a href="https://scholar.google.ch/citations?user=P-op4CgAAAAJ&hl=de">Corinne Stucker</a>,
          <a href="https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>,
        <br>
        <em>arXiv</em>, 2020
        <br>
        <a href="https://arxiv.org/abs/2012.07427">paper</a> 
        <p></p>
        <p>

          This work presents a method for automatically refining 3D city models generated from aerial images by using a neural network trained with reference data and a loss function to improve DSMs,
          effectively preserving geometric structures while removing noise and artifacts.
        </p>
      </td>
    </tr>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>News</h2>
                  <ul>
				    <li>20th of November 2025: I am giving a talk at ZurichCV.</li>
				    <li>October 2025: I am attending ICCV, presenting Marigold-DC.</li>
				    <li>April 2025: I started as a Student Researcher at Google in the Team of <a href="https://federicotombari.github.io/">Federico Tombari</a>.</li>
				    <li>17th March 2025: Presentation at <a href="https://eth4d.ethz.ch/news-and-events/events/humanitarian-action-in-the-digital-age.html">Humanitarian Action in the Digital Age</a>.</li>
				    <li>12th March 2025: Presenter at Helveta's internal Big Data Webinar. </li>
				    <li>29th November 2024: Presenting POPCORN and POMELO at the ETH4D General Assembly.</li>
				    <li>October 2024: I am attending ECCV in Milano, let me know if you think we should meet :)</li>
                    <li>16th - 21st of June 2024: I'm attending <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a> in Seattle, WA, USA presenting Marigold.</li>
                    <li>6th June 2024: Invited talk for the KoRaTo Team of University of Ulm at ETH Zurich.</li>
                    <li>25th April 2024: Invited talk about my population mapping projects at <a href="https://www.worldpop.org/">WorldPop</a> in Southampton, UK.</li>
                    <li>December 2023 - February 2024: I'm interning at Meta's Reality Labs in Redmond, WA, USA.</li>
                    <li>22nd of September 2023: Invited talk at UZH Astrophysics Seminar. <a href="https://www.youtube.com/watch?v=0Mq7OUHbfow">Recording</a> </li>
                    <li>4th - 6th of September 2023: I'm attending the <a href="https://swiss-remote-sensing-days.github.io/">Swiss Remote Sensing Days 2023</a> in St. Gallen, CH. </li>
                    <li>15th of August 2023: Invited talk at Google's Semantic perception group.</li>
                    <li>10th of August 2023: Invited talk at Google's Open Building team.</li>
                    <li>1st of July 2023: Invited talk at <a href="https://socialincome.org/">SocialIncome</a> to present the POMELO project. </li>
                    <li>18th - 22nd of June 2023: I am attending <a href="https://cvpr.thecvf.com/Conferences/2023">CVPR 2023</a> in Vancouver, Canada presenting DADA.</li>
                    <li>13th of March 2023: I am visiting the ICRC headquarters in Geneva, CH.</li>
                    <li>13th of March 2023: Invited talk about POMELO at the SDG lab workshop at Deloitte in Geneva, CH.</li>
                    <li>30th of November 2022: Invited talk at ETH's Weather and Climate Risks group.</li>
                    <li>14th of October 2022: Invited talk at the <a href="https://ai.ethz.ch/industry/AIplusX/aixsummit22.html">ETH's AI+X event </a>, presenting POMELO.</li>
                    <li>28th of June - 1st of July 2022: I'm at the Machine Learning Summer School in Krakow, Poland. </li>
                    <li>23rd - 27th of May 2022: I'm attending <a href="https://earth.esa.int/eogateway/events/living-planet-symposium-2022">ESA's Living Planet Symposium</a> in Bonn, DE.</li>
                    <li>1st - 4th of May 2022: I'm attending the <a href="https://www.epfl.ch/research/domains/eo/events-and-news/srsd-2022/">Swiss Remote Sensing Days 2022</a> in Ascona, CH.</li>
                    <li>1st of September 2021: I started my PhD at the <a href="https://prs.igp.ethz.ch/">Photogrammetry and Remote Sensing Lab at ETH Zurich.</a> </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Thank you for the template <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
